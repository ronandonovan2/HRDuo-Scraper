---
phase: 01-scraper-pipeline
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - package.json
  - .gitignore
  - .env.example
  - src/config.js
  - src/scraper.js
  - src/index.js
  - data/jobs.json
autonomous: false

must_haves:
  truths:
    - "Running `node src/index.js` locally produces a data/jobs.json file containing job objects"
    - "Each job object in jobs.json has title, type, description, and applyUrl string fields"
    - "The scraper launches Playwright Chromium, navigates to the HR Duo SPA, and waits for JavaScript-rendered job content before extracting"
    - "If zero jobs are found (HR Duo down or scraper failure), existing jobs.json is NOT overwritten — scraper exits with a non-zero code and warning message"
    - "Selectors used for scraping are defined in a single config file so they can be updated without changing scraper logic"
  artifacts:
    - path: "package.json"
      provides: "Project manifest with playwright dependency"
      contains: "playwright"
    - path: "src/config.js"
      provides: "Scraper configuration — HR Duo URL, CSS selectors, timeout values"
      exports: ["config"]
    - path: "src/scraper.js"
      provides: "Playwright scraping logic — launches browser, navigates to HR Duo, extracts job data"
      exports: ["scrapeJobs"]
    - path: "src/index.js"
      provides: "Orchestrator — calls scraper, validates results, writes jobs.json"
    - path: "data/jobs.json"
      provides: "Output file containing array of job objects"
    - path: ".gitignore"
      provides: "Excludes node_modules, .env, playwright browser cache"
  key_links:
    - from: "src/index.js"
      to: "src/scraper.js"
      via: "import scrapeJobs function"
      pattern: "require.*scraper"
    - from: "src/index.js"
      to: "src/config.js"
      via: "import config object"
      pattern: "require.*config"
    - from: "src/scraper.js"
      to: "https://my.hrduo.com/candidate-jobs/Croom_Medical"
      via: "page.goto with config.url"
      pattern: "page\\.goto"
    - from: "src/index.js"
      to: "data/jobs.json"
      via: "fs.writeFileSync after validation"
      pattern: "writeFileSync.*jobs\\.json"
---

<objective>
Build a Playwright scraper that loads the HR Duo candidate portal SPA, extracts job listings (title, type, description, apply URL), validates the data, and writes it to data/jobs.json. Includes zero-result safety to prevent publishing empty data when the scraper fails.

Purpose: This is the core data extraction capability — everything else in the project depends on having valid job data in a JSON file.
Output: A Node.js project with a working scraper that produces data/jobs.json when run locally.
</objective>

<execution_context>
@C:/Users/ronan.donovan/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/ronan.donovan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/research/STACK.md
@.planning/research/PITFALLS.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Node.js project with Playwright scraper, config, and jobs.json output</name>
  <files>
    package.json
    .gitignore
    .env.example
    src/config.js
    src/scraper.js
    src/index.js
  </files>
  <action>
    Initialize a Node.js project and build the complete scraper pipeline:

    **package.json:**
    - name: "hrduo-scraper"
    - type: "module" is NOT needed — use CommonJS (require/module.exports) for simplicity
    - scripts: `"scrape": "node src/index.js"`
    - dependencies: playwright (install via npm)
    - After creating package.json, run `npm install playwright` and `npx playwright install chromium` (Chromium only — smaller footprint per STACK.md recommendation)

    **.gitignore:**
    - node_modules/
    - .env
    - playwright browser binaries (if stored locally)

    **.env.example:**
    - No secrets needed for Phase 1 (scraper reads a public page)
    - Include placeholder: `HRDUO_URL=https://my.hrduo.com/candidate-jobs/Croom_Medical`

    **src/config.js:**
    - Export a config object with:
      - `url`: HR Duo candidate portal URL (`https://my.hrduo.com/candidate-jobs/Croom_Medical`)
      - `selectors`: Object with CSS selector strings for: jobCard, title, type, description, applyLink. Use placeholder selectors initially since HR Duo is in maintenance — mark each with a `// TODO: verify selector against live page` comment. Start with reasonable guesses based on common SPA job board patterns:
        - jobCard: `'.job-card'` (fallback: `'[class*="job"]'`)
        - title: `'h2, h3, [class*="title"]'`
        - type: `'[class*="type"], [class*="category"]'`
        - description: `'[class*="description"], [class*="summary"], p'`
        - applyLink: `'a[href*="candidate-jobs"]'`
      - `timeout`: 30000 (30 seconds for SPA to render)
      - `maxRetries`: 2

    **src/scraper.js:**
    - Export async function `scrapeJobs(config)` that:
      1. Launches Playwright Chromium in headless mode
      2. Creates a new page with a reasonable viewport (1280x720)
      3. Navigates to `config.url` with `waitUntil: 'networkidle'` (SPA needs JS to finish loading)
      4. Waits for job card selector to appear (with config.timeout), wrapped in try/catch
      5. If no job cards found after timeout, logs a warning and returns empty array (does NOT throw)
      6. Queries all job card elements using `config.selectors.jobCard`
      7. For each card, extracts: title (text content from title selector), type (text content from type selector), description (text content from description selector, truncated to 200 chars), applyUrl (href from applyLink selector — if relative, resolve against base URL)
      8. Returns array of job objects: `[{ title, type, description, applyUrl }]`
      9. Always closes browser in a finally block
      10. Logs progress: "Launching browser...", "Navigating to HR Duo...", "Found N job cards", "Extraction complete"
    - Use console.log for logging (winston is overkill for this project size)

    **src/index.js:**
    - Main orchestrator that:
      1. Imports config from ./config.js and scrapeJobs from ./scraper.js
      2. Calls `scrapeJobs(config)`
      3. Validates results: each job must have non-empty title and applyUrl (type and description can be empty strings but must exist as keys)
      4. Filters out any invalid jobs, logs warnings for each
      5. **Zero-result safety:** If valid jobs array is empty:
         - Check if `data/jobs.json` already exists
         - If it does, log WARNING: "Scraper found 0 valid jobs. Preserving existing jobs.json to prevent data loss."
         - Exit with process.exit(1) — this signals to CI that something went wrong
         - If no existing jobs.json, write empty array (first run scenario)
      6. If valid jobs found, write to `data/jobs.json` with JSON.stringify(jobs, null, 2)
      7. Log: "Successfully wrote N jobs to data/jobs.json"
      8. Ensure `data/` directory exists before writing (use fs.mkdirSync with recursive: true)
  </action>
  <verify>
    1. `npm run scrape` executes without crash (may find 0 jobs if HR Duo is in maintenance — that is expected)
    2. `node -e "const c = require('./src/config.js'); console.log(c.config.url)"` prints the HR Duo URL
    3. `node -e "const s = require('./src/scraper.js'); console.log(typeof s.scrapeJobs)"` prints "function"
    4. If HR Duo is live: `data/jobs.json` exists and contains a valid JSON array
    5. If HR Duo is in maintenance: scraper logs warning and exits with code 1, no jobs.json overwrite
  </verify>
  <done>
    - package.json exists with playwright dependency installed
    - src/config.js exports config with url, selectors, timeout
    - src/scraper.js exports scrapeJobs that uses Playwright to load HR Duo SPA
    - src/index.js orchestrates scrape -> validate -> write with zero-result safety
    - Running `npm run scrape` produces meaningful log output (success or controlled failure)
    - .gitignore excludes node_modules and .env
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 2: Verify scraper output against live HR Duo page</name>
  <files>src/config.js, data/jobs.json</files>
  <action>
    Human verification checkpoint. The scraper has been built in Task 1. Now the user must verify it works against the live HR Duo page and adjust selectors if needed.

    What was built: Playwright scraper that loads the HR Duo candidate portal SPA and extracts job listings to data/jobs.json. Includes configurable selectors in src/config.js and zero-result safety in src/index.js.

    How to verify:
    1. Open https://my.hrduo.com/candidate-jobs/Croom_Medical in your browser
    2. If the page is still in maintenance: this checkpoint is BLOCKED — note it and move to Plan 01-02. Return to this checkpoint when HR Duo is back up.
    3. If the page is live with job listings:
       a. Right-click a job card and Inspect Element
       b. Note the actual CSS selectors for: job card container, title, type, description, apply link
       c. Update src/config.js selectors to match the real DOM structure
       d. Run `npm run scrape`
       e. Open data/jobs.json and verify:
          - Array contains the same number of jobs as the page
          - Each job has title, type, description, applyUrl
          - applyUrl points to the correct HR Duo individual job page
          - Description is a reasonable summary (not HTML, not empty)
    4. Try running `npm run scrape` twice — second run should also succeed (idempotent)

    Resume signal: Type "approved" if jobs.json matches the live page, OR describe what needs fixing (wrong selectors, missing data, etc.). If HR Duo is in maintenance, type "blocked:maintenance" to defer and proceed to Plan 01-02.
  </action>
  <verify>User confirms jobs.json output matches live HR Duo page content, or acknowledges maintenance blocker</verify>
  <done>Scraper selectors are validated against real DOM and jobs.json contains accurate job data from HR Duo</done>
</task>

</tasks>

<verification>
- `npm run scrape` completes without unhandled errors
- `data/jobs.json` is valid JSON (parseable by `JSON.parse`)
- Each job object has exactly these keys: title, type, description, applyUrl
- Zero-result safety: if `data/jobs.json` exists and scraper finds 0 jobs, file is NOT overwritten
- No credentials or secrets in committed code
- node_modules and .env are gitignored
</verification>

<success_criteria>
Running `npm run scrape` on a machine with Node.js installed produces a valid `data/jobs.json` containing one object per HR Duo job listing, with title, type, description, and applyUrl fields. If HR Duo is unreachable, the existing jobs.json is preserved.
</success_criteria>

<output>
After completion, create `.planning/phases/01-scraper-pipeline/01-01-SUMMARY.md`
</output>
