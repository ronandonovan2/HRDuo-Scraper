---
phase: 01-scraper-pipeline
plan: 02
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - .github/workflows/scrape.yml
  - data/.gitkeep
autonomous: false

must_haves:
  truths:
    - "GitHub Actions workflow runs on a daily cron schedule (once per day) without manual trigger"
    - "The workflow installs Node.js, Playwright Chromium with system deps, and runs the scraper"
    - "After a successful scrape, the updated jobs.json is committed and pushed to the repo automatically"
    - "jobs.json is publicly accessible at a stable, predictable raw GitHub URL"
    - "If the scraper exits with non-zero (zero jobs found), the workflow does NOT commit — existing jobs.json is preserved"
    - "The workflow can also be triggered manually via workflow_dispatch for testing"
  artifacts:
    - path: ".github/workflows/scrape.yml"
      provides: "GitHub Actions workflow definition with cron schedule, Playwright setup, scraper execution, and auto-commit"
      contains: "schedule"
    - path: "data/.gitkeep"
      provides: "Ensures data/ directory exists in repo before first scrape"
  key_links:
    - from: ".github/workflows/scrape.yml"
      to: "src/index.js"
      via: "npm run scrape step"
      pattern: "npm run scrape"
    - from: ".github/workflows/scrape.yml"
      to: "data/jobs.json"
      via: "git add + git commit + git push after scrape"
      pattern: "git.*commit.*jobs\\.json"
---

<objective>
Create a GitHub Actions workflow that runs the Playwright scraper on a daily cron schedule, commits the updated jobs.json to the repository, and makes it publicly accessible at a stable raw GitHub URL. Includes safety: only commits if the scraper succeeds (non-empty results).

Purpose: Automates the scraper so job data stays current without manual intervention. The publicly accessible jobs.json becomes the data source for the careers page in Phase 2.
Output: `.github/workflows/scrape.yml` — a working CI workflow that runs daily and publishes fresh job data.
</objective>

<execution_context>
@C:/Users/ronan.donovan/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/ronan.donovan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-scraper-pipeline/01-01-SUMMARY.md
@.planning/research/STACK.md
@.planning/research/PITFALLS.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create GitHub Actions workflow for daily scraping and auto-commit</name>
  <files>
    .github/workflows/scrape.yml
    data/.gitkeep
  </files>
  <action>
    Create the GitHub Actions workflow and ensure the data directory is tracked:

    **data/.gitkeep:**
    - Empty file to ensure `data/` directory exists in the repo before the first scrape run

    **.github/workflows/scrape.yml:**
    Create a workflow with these specifications:

    ```yaml
    name: Scrape HR Duo Jobs

    on:
      schedule:
        - cron: '0 7 * * *'    # Daily at 7:00 AM UTC (morning before business hours in Ireland)
      workflow_dispatch:         # Allow manual trigger for testing

    permissions:
      contents: write            # Needed to commit and push jobs.json

    jobs:
      scrape:
        runs-on: ubuntu-latest
        steps:
          - name: Checkout repository
            uses: actions/checkout@v4

          - name: Set up Node.js
            uses: actions/setup-node@v4
            with:
              node-version: '20'
              cache: 'npm'

          - name: Install dependencies
            run: npm ci

          - name: Install Playwright Chromium
            run: npx playwright install --with-deps chromium
            # --with-deps installs system-level dependencies (libgbm, libnss, etc.)
            # Per PITFALLS.md: this is critical for CI — Playwright fails without system deps

          - name: Run scraper
            run: npm run scrape
            # If scraper finds 0 jobs, it exits with code 1 (zero-result safety from Plan 01-01)
            # This causes the step to fail, preventing the commit step from running
            # Existing jobs.json in the repo is preserved

          - name: Commit and push updated jobs.json
            run: |
              git config user.name "github-actions[bot]"
              git config user.email "github-actions[bot]@users.noreply.github.com"
              git add data/jobs.json
              # Only commit if there are actual changes (git diff --cached checks staging area)
              if git diff --cached --quiet; then
                echo "No changes to jobs.json — skipping commit"
              else
                git commit -m "data: update jobs.json [skip ci]"
                git push
              fi
            # [skip ci] in commit message prevents infinite workflow loop
    ```

    **Design decisions:**
    - Single daily run at 7 AM UTC (8 AM Irish time during GMT+1) — jobs are updated before business hours
    - `workflow_dispatch` enables manual testing without waiting for cron
    - `npm ci` (not `npm install`) for reproducible CI builds
    - `actions/setup-node@v4` with npm cache for faster runs
    - `npx playwright install --with-deps chromium` — Chromium only (not all browsers) per STACK.md
    - Zero-result safety is handled by the scraper itself (exits non-zero) — no extra logic needed in the workflow
    - `git diff --cached --quiet` prevents empty commits when jobs haven't changed
    - `[skip ci]` in commit message prevents the commit from triggering another workflow run
    - Bot user for commits keeps the git history clean

    **Public URL for jobs.json:**
    After the workflow runs, jobs.json will be accessible at:
    `https://raw.githubusercontent.com/{owner}/{repo}/main/data/jobs.json`
    (Replace {owner}/{repo} with actual GitHub org/repo name)

    This URL is stable and cacheable. Phase 2's JavaScript snippet will fetch from this URL.
  </action>
  <verify>
    1. `.github/workflows/scrape.yml` is valid YAML — run: `node -e "const yaml = require('yaml'); const fs = require('fs'); yaml.parse(fs.readFileSync('.github/workflows/scrape.yml', 'utf8')); console.log('Valid YAML')"` (install yaml package temporarily if needed, or just check syntax)
    2. Workflow file contains `schedule`, `workflow_dispatch`, `playwright install --with-deps`, `npm run scrape`, and `git commit` steps
    3. `data/.gitkeep` file exists
    4. Workflow uses `actions/checkout@v4` and `actions/setup-node@v4`
    5. Commit message includes `[skip ci]` to prevent loops
  </verify>
  <done>
    - .github/workflows/scrape.yml exists with cron schedule, Playwright setup, scraper execution, and auto-commit logic
    - data/.gitkeep ensures data directory is tracked
    - Workflow only commits when jobs.json actually changes
    - Workflow fails gracefully (no commit) when scraper finds zero jobs
    - Manual trigger available via workflow_dispatch
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 2: Verify GitHub Actions workflow runs and publishes jobs.json</name>
  <files>.github/workflows/scrape.yml, data/jobs.json</files>
  <action>
    Human verification checkpoint. The GitHub Actions workflow has been created in Task 1. Now the user must push to GitHub, trigger the workflow, and verify jobs.json is published.

    What was built: GitHub Actions workflow (.github/workflows/scrape.yml) that runs the Playwright scraper daily at 7 AM UTC, commits updated jobs.json to the repo, and makes it publicly accessible. Includes manual trigger via workflow_dispatch and zero-result safety (no commit on scraper failure).

    How to verify:
    1. Push the repository to GitHub (if not already done)
    2. Go to the repository's Actions tab on GitHub
    3. Find the "Scrape HR Duo Jobs" workflow
    4. Click "Run workflow" (manual trigger) to test it
    5. Watch the workflow run:
       a. All steps should pass (green checkmarks) — IF HR Duo is live with jobs
       b. If HR Duo is in maintenance: the "Run scraper" step will fail (expected) and no commit happens
    6. If the workflow succeeded:
       a. Check the repo — a new commit by github-actions[bot] should appear with updated data/jobs.json
       b. Navigate to https://raw.githubusercontent.com/{owner}/{repo}/main/data/jobs.json
       c. Verify the JSON is accessible and contains job data
    7. Run the workflow a second time — if jobs haven't changed, it should skip the commit ("No changes to jobs.json")

    Resume signal: Type "approved" if the workflow ran successfully and jobs.json is publicly accessible. If HR Duo is in maintenance, type "blocked:maintenance". Describe any issues if the workflow failed unexpectedly.
  </action>
  <verify>User confirms GitHub Actions workflow ran successfully and jobs.json is accessible at raw GitHub URL</verify>
  <done>Daily scraper pipeline is operational: workflow triggers on cron, scrapes HR Duo, and publishes jobs.json to a public URL</done>
</task>

</tasks>

<verification>
- `.github/workflows/scrape.yml` is valid YAML with correct GitHub Actions syntax
- Workflow triggers on: daily cron schedule AND manual workflow_dispatch
- Workflow installs Node.js 20, npm dependencies, and Playwright Chromium with system deps
- Workflow runs `npm run scrape` and commits data/jobs.json only on success
- Workflow does NOT commit when scraper fails (zero-result safety)
- Workflow does NOT commit when jobs.json is unchanged (idempotent)
- Commit message includes `[skip ci]` to prevent workflow loops
- After successful run, jobs.json is accessible at raw.githubusercontent.com URL
</verification>

<success_criteria>
The GitHub Actions workflow runs successfully on manual trigger, executes the Playwright scraper, commits updated jobs.json to the repo, and the file is publicly accessible at a stable raw GitHub URL. The workflow also runs daily on cron without manual intervention.
</success_criteria>

<output>
After completion, create `.planning/phases/01-scraper-pipeline/01-02-SUMMARY.md`
</output>
